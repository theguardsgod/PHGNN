{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00137f13-243c-477f-a41b-d864ec88186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from data_declaration import Task\n",
    "from loader_helper    import LoaderHelper\n",
    "\n",
    "\n",
    "from vapformer.model_components import thenet,UnetrPP\n",
    "from evaluation import evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "from vapformer.dynunet_block import get_conv_layer, UnetResBlock\n",
    "from monai.networks.layers.utils import get_norm_layer\n",
    "\n",
    "from sklearn import neighbors\n",
    "import scipy.sparse as sp\n",
    "from models import tabular_net\n",
    "from torch_geometric.data import Data,Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c109f-e65d-40fe-90d6-f2629eb9e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:1\")\n",
    "\n",
    "ld_helper = LoaderHelper(task=Task.NC_v_AD)\n",
    "ld_helper = LoaderHelper(task=Task.sMCI_v_pMCI)\n",
    "train_dl = ld_helper.get_train_dl(0, batch_size = 16)\n",
    "test_dl = ld_helper.get_test_dl(0, batch_size = 64, shuffle=False)\n",
    "\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83923c-438d-4583-9f72-8481804f3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "def sen_spe(a, b):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    for i in range(len(a)):  # a:label\n",
    "        if a[i] == 1 and b[i] == 1:\n",
    "            TP = TP + 1\n",
    "        elif a[i] == 1 and b[i] == 0:\n",
    "            FN = FN + 1\n",
    "        elif a[i] == 0 and b[i] == 1:\n",
    "            FP = FP + 1\n",
    "        elif a[i] == 0 and b[i] == 0:\n",
    "            TN = TN + 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    TPR = TP / (TP + FN + 1e-6)  # True positive rate, Sensitivity\n",
    "    TNR = TN / (TN + FP + 1e-6)  # True Negative Rate, Specificity\n",
    "    return TPR, TNR\n",
    "def test_GNN(model, mask, tensor_x, adj):\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(tensor_x.to(DEVICE).float(), adj.to(DEVICE).float())\n",
    "    mask_logits = logits[mask]\n",
    "    \n",
    "    predicted = mask_logits.max(dim=1)[1].long()\n",
    "    label = labels[mask]\n",
    "\n",
    "    predicted = predicted.cpu().detach().numpy()\n",
    "    label = label.cpu().detach().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(label, predicted)\n",
    "    sensitivity, specificity = sen_spe(label, predicted)\n",
    "    \n",
    "\n",
    "    auc = roc_auc_score(label, mask_logits.cpu().detach())\n",
    "\n",
    "    return accuracy, sensitivity, specificity, auc\n",
    "def evaluate_gnn(total_label, total_pre):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = 0.000001\n",
    "    TN = 0.000001\n",
    "    FP = 0.000001\n",
    "    FN = 0.000001\n",
    "    fpr, tpr, thresholds = roc_curve(total_label, total_pre[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # 在ROC曲线上选择最佳阈值\n",
    "    best_idx = np.argmax(tpr - fpr)\n",
    "\n",
    "    best_thresh_roc = thresholds[best_idx]\n",
    "    print(best_thresh_roc)\n",
    "\n",
    "    for i in range(len(total_pre)):\n",
    "                real_class = total_label[i]\n",
    "                predicted_class = 1 if total_pre[i, 1] > best_thresh_roc else 0\n",
    "                \n",
    "                if predicted_class == real_class:\n",
    "                    correct += 1\n",
    "                    if real_class == 0:\n",
    "                        TN += 1\n",
    "                    elif real_class == 1:\n",
    "                        TP += 1\n",
    "                else:\n",
    "                    if real_class == 0:\n",
    "                        FP += 1\n",
    "                    elif real_class == 1:\n",
    "                        FN += 1\n",
    "                    \n",
    "                total += 1\n",
    "    \n",
    "    sensitivity = round((TP / (TP + FN)), 5)\n",
    "    specificity = round((TN / (TN + FP)), 5)\n",
    "    accuracy = round((sensitivity + specificity) / 2, 5)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da8acb-b8aa-4c43-802b-826951132dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_embedding(embedding, name, n = 30):\n",
    "    latent_dim, batch_size = embedding.shape\n",
    "    if name == 'knn':\n",
    "        A = neighbors.kneighbors_graph(embedding, n_neighbors = n).toarray()\n",
    "        A = (A + np.transpose(A)) / 2\n",
    "        return A\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e4712-ead5-4b4d-b280-f073dfe1c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_resnet18 = torch.load('./weights/mri_resnet50_mci_new.pth').to(DEVICE)\n",
    "pet_resnet18 = torch.load('./weights/pet_resnet50_mci.pth').to(DEVICE)\n",
    "tab_net = torch.load('./weights/tb_net_MCI.pth').to(DEVICE)\n",
    "mri_resnet18.fc = torch.nn.Identity()\n",
    "pet_resnet18.fc = torch.nn.Identity()\n",
    "tab_net.linear3 = torch.nn.Identity()\n",
    "#resnet18.sig = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c2ee9-8898-4e79-a4a5-c7ad04b2ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# 假设 train_dl 和 test_dl 是你的训练和测试数据加载器\n",
    "\n",
    "# 创建一个空的 ConcatDataset\n",
    "combined_dataset = ConcatDataset([train_dl.dataset, test_dl.dataset])\n",
    "\n",
    "# 创建合并后的 DataLoader\n",
    "combined_dl = DataLoader(combined_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 初始化一个空的 train_mask\n",
    "train_mask = []\n",
    "\n",
    "# 遍历 combined_dataset 数据集\n",
    "train_len = len(train_dl.dataset)\n",
    "test_len = len(test_dl.dataset)\n",
    "\n",
    "for i in range(train_len + test_len):\n",
    "    if i < train_len:\n",
    "        train_mask.append(True)  # 训练集的样本\n",
    "    else:\n",
    "        train_mask.append(False)  # 测试集的样本\n",
    "\n",
    "# 将 train_mask 转换为 Torch Tensor\n",
    "train_mask = torch.tensor(train_mask)\n",
    "\n",
    "# 打印 mask 长度以验证\n",
    "print(f\"Length of train_mask: {len(train_mask)}\")\n",
    "print(f\"Number of True values in train_mask: {train_mask.sum().item()}\")\n",
    "\n",
    "# 确保 train_mask 长度与 combined_dataset 长度一致\n",
    "assert len(train_mask) == len(combined_dataset)\n",
    "\n",
    "# 现在 train_mask 是一个 Torch Tensor，每个元素表示相应样本是否来自训练集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a90542-830a-4d50-9796-23b0531386fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "label = []\n",
    "# 将模型设置为评估模式\n",
    "\n",
    "mri_resnet18.eval()\n",
    "pet_resnet18.eval()\n",
    "tab_net.eval()\n",
    "for _, sample_batched in enumerate(tqdm(combined_dl)):\n",
    "    batch_mri = sample_batched['mri'].to(DEVICE).float()\n",
    "    batch_pet = sample_batched['pet'].to(DEVICE).float()\n",
    "    batch_clinical = sample_batched['clin_t'].to(DEVICE).float()\n",
    "    batch_clinical = torch.nan_to_num(batch_clinical, nan=0.0)\n",
    "    with torch.no_grad():\n",
    "        mri_outputs = mri_resnet18(batch_mri)\n",
    "        pet_outputs = pet_resnet18(batch_pet)\n",
    "        tab_outputs = tab_net(batch_clinical)\n",
    "        outputs = torch.cat([mri_outputs, pet_outputs, tab_outputs], dim=1)\n",
    "        #outputs = torch.cat([mri_outputs], dim=1)\n",
    "    all_outputs.append(outputs)\n",
    "    label.append(sample_batched['label'])\n",
    "    labels = torch.cat(label, dim=0)\n",
    "    stacked_outputs = torch.cat(all_outputs, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914a500-9760-4c28-9f50-709db120af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypergraph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb768bc0-d118-4766-af2d-cab83f3568b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_mat = Eu_dis(stacked_outputs.cpu())\n",
    "H = construct_H_with_KNN_from_distance(dis_mat, 50, is_probH=False, m_prob=1)\n",
    "G = generate_G_from_H(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5e479-9775-4a27-a196-06369b3d4c65",
   "metadata": {},
   "source": [
    "# GraphMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc0d3e-fae2-42e5-a263-37f8afa0c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sce_loss(x, y, alpha=3):\n",
    "    x = F.normalize(x, p=2, dim=-1)\n",
    "    y = F.normalize(y, p=2, dim=-1)\n",
    "    loss = (1 - (x * y).sum(dim=-1)).pow_(alpha)\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "# loss function: sig\n",
    "def sig_loss(x, y):\n",
    "    x = F.normalize(x, p=2, dim=-1)\n",
    "    y = F.normalize(y, p=2, dim=-1)\n",
    "    loss = (x * y).sum(1)\n",
    "    loss = torch.sigmoid(-loss)\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def mask_edge(graph, mask_prob):\n",
    "    E = graph.num_edges()\n",
    "    mask_rates = torch.FloatTensor(np.ones(E) * mask_prob)\n",
    "    masks = torch.bernoulli(1 - mask_rates)\n",
    "    mask_idx = masks.nonzero().squeeze(1)\n",
    "    return mask_idx\n",
    "\n",
    "\n",
    "# graph transformation: drop edge\n",
    "def drop_edge(graph, drop_rate, return_edges=False):\n",
    "    if drop_rate <= 0:\n",
    "        return graph\n",
    "    edge_mask = mask_edge(graph, drop_rate)\n",
    "    src = graph.edges()[0]\n",
    "    dst = graph.edges()[1]\n",
    "\n",
    "    nsrc = src[edge_mask]\n",
    "    ndst = dst[edge_mask]\n",
    "\n",
    "    ng = Data(edge_index=torch.concat((nsrc, ndst), 0))\n",
    "    dsrc = src[~edge_mask]\n",
    "    ddst = dst[~edge_mask]\n",
    "\n",
    "    if return_edges:\n",
    "        return ng, (dsrc, ddst)\n",
    "    return ng\n",
    "\n",
    "def initialize_gnn_decoder(gnn_type, input_dim, hid_dim, num_layer,device):\n",
    "    if gnn_type == 'GAT':\n",
    "            gnn = GAT(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    elif gnn_type == 'GCN':\n",
    "            gnn = GCN(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    elif gnn_type == 'GraphSAGE':\n",
    "            gnn = GraphSAGE(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    elif gnn_type == 'GIN':\n",
    "            gnn = GIN(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    elif gnn_type == 'GCov':\n",
    "            gnn = GCov(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    elif gnn_type == 'GraphTransformer':\n",
    "            gnn = GraphTransformer(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    elif gnn_type == 'HGNN':\n",
    "            gnn = HGNN(input_dim = input_dim, hid_dim = hid_dim, num_layer = num_layer)\n",
    "    else:\n",
    "            raise ValueError(f\"Unsupported GNN type: {gnn_type}\")\n",
    "    gnn.to(device)\n",
    "    return gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7e818-b487-40b7-9caf-6efab83020c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class GraphMAELoss(nn.Module):\n",
    "    def __init__(self, encoder, decoder, hidden_dim, enc_in_dim, dec_in_dim, mask_rate=0.75, drop_edge_rate=0.0, replace_rate=0.1, loss_fn='sce', alpha_l=2):\n",
    "        super(GraphMAELoss, self).__init__()\n",
    "        self._mask_rate = mask_rate\n",
    "        self._drop_edge_rate = drop_edge_rate\n",
    "        self._replace_rate = replace_rate\n",
    "        self._mask_token_rate = 1 - self._replace_rate\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # build encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # build decoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.enc_mask_token = nn.Parameter(torch.zeros(1, enc_in_dim))\n",
    "        self.encoder_to_decoder = nn.Linear(dec_in_dim, dec_in_dim, bias=False)\n",
    "        # setup loss function\n",
    "        self.criterion = self.setup_loss_fn(loss_fn, alpha_l)\n",
    "\n",
    "    def forward(self, data):\n",
    "              \n",
    "        loss, x_hidden = self.mask_attr_prediction(data)\n",
    "        loss_item = {\"loss\": loss.item()}\n",
    "\n",
    "        return loss, loss_item,x_hidden\n",
    "\n",
    "    def setup_loss_fn(self, loss_fn, alpha_l):\n",
    "        if loss_fn == \"mse\":\n",
    "            criterion = nn.MSELoss()\n",
    "        elif loss_fn == \"sce\":\n",
    "            criterion = partial(sce_loss, alpha=alpha_l)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return criterion\n",
    "\n",
    "    def encoding_mask_noise(self, g, x, mask_rate=0.3):\n",
    "        num_nodes = g.num_nodes\n",
    "        \n",
    "        perm = torch.randperm(num_nodes, device=x.device)\n",
    "        # random masking\n",
    "        num_mask_nodes = int(mask_rate * num_nodes)\n",
    "        mask_nodes = perm[: num_mask_nodes]\n",
    "        keep_nodes = perm[num_mask_nodes:]\n",
    "        if self._replace_rate > 0:\n",
    "            num_noise_nodes = int(self._replace_rate * num_mask_nodes)\n",
    "            perm_mask = torch.randperm(num_mask_nodes, device=x.device)\n",
    "            token_nodes = mask_nodes[perm_mask[: int(self._mask_token_rate * num_mask_nodes)]]\n",
    "            noise_nodes = mask_nodes[perm_mask[-int(self._replace_rate * num_mask_nodes):]]\n",
    "            noise_to_be_chosen = torch.randperm(num_nodes, device=x.device)[:num_noise_nodes]\n",
    "            out_x = x.clone()\n",
    "            out_x[token_nodes] = 0.0\n",
    "            out_x[noise_nodes] = x[noise_to_be_chosen]\n",
    "        else:\n",
    "            out_x = x.clone()\n",
    "            token_nodes = mask_nodes\n",
    "            out_x[mask_nodes] = 0.0\n",
    "        out_x[token_nodes] += self.enc_mask_token\n",
    "        use_g = g.clone()\n",
    "        return use_g, out_x, (mask_nodes, keep_nodes)\n",
    "    \n",
    "    def mask_attr_prediction(self, data, pretrain_method='graphmae'):\n",
    "        \n",
    "        g = data\n",
    "        x = data.x\n",
    "\n",
    "        pre_use_g, use_x, (mask_nodes, keep_nodes) = self.encoding_mask_noise(g, x, self._mask_rate)\n",
    "        \n",
    "        if self._drop_edge_rate > 0:\n",
    "            use_g, masked_edges = drop_edge(pre_use_g, self._drop_edge_rate, return_edges=True)\n",
    "        else:\n",
    "            use_g = pre_use_g\n",
    "        \n",
    "        # if there are noise nodes before reconstruction, then execture this line\n",
    "        all_hidden = self.encoder(x=use_x, edge_index=use_g.edge_index)\n",
    "\n",
    "        # if there are none noise nodes before reconstruction, please execture this line\n",
    "        # all_hidden = self.encoder(data.x, data.edge_index)\n",
    "\n",
    "        # ---- attribute reconstruction ----\n",
    "\n",
    "        node_reps = self.encoder_to_decoder(all_hidden)\n",
    "        node_reps[mask_nodes] = 0\n",
    "\n",
    "        recon_graph = Data(x=node_reps, edge_index=pre_use_g.edge_index).to(data.x.device)\n",
    "        recon_node_reps = self.decoder(recon_graph.x, recon_graph.edge_index)\n",
    "\n",
    "        x_init = x[mask_nodes]\n",
    "        x_rec = recon_node_reps[mask_nodes]\n",
    "        loss = self.criterion(x_rec, x_init)\n",
    "        return loss, all_hidden\n",
    "\n",
    "    def embed(self, g, x):\n",
    "        rep = self.encoder(x=x, edge_index=g.edge_index)\n",
    "        return rep\n",
    "\n",
    "    @property\n",
    "    def enc_params(self):\n",
    "        return self.encoder.parameters()\n",
    "\n",
    "    @property\n",
    "    def dec_params(self):\n",
    "        return chain(*[self.encoder_to_decoder.parameters(), self.decoder.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2867a-9d35-4909-8c17-934797a8151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=stacked_outputs.to(DEVICE), edge_index=torch.Tensor(G).to(DEVICE), y=labels.squeeze().long(), train_mask=train_mask, val_mask = ~train_mask, test_mask = ~train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eaae38-519b-4520-8d80-97a85217de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = np.transpose(np.transpose(np.nonzero(H)))\n",
    "data = Data(x=stacked_outputs.to(DEVICE), edge_index=torch.Tensor(edge_indices).long().to(DEVICE), y=labels.squeeze().long(), train_mask=train_mask, val_mask = ~train_mask, test_mask = ~train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b10ce-ccab-4df7-9b62-c3a2930e7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import sys\n",
    "sys.path.append('/home/cyliu/code/ProG/')\n",
    "from prompt_graph.data import load4link_prediction_multi_graph, load4link_prediction_single_graph\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from prompt_graph.model import GAT, GCN, GCov, GIN, GraphSAGE, GraphTransformer, HGNN\n",
    "from prompt_graph.data import load4node, load4graph, NodePretrain\n",
    "import os\n",
    "\n",
    "\n",
    "data = data.to(DEVICE)\n",
    "input_dim = data.x.shape[1]\n",
    "output_dim = 2\n",
    "in_node_feat_dim = input_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db998fe-3b4a-4cfd-a96a-e00dd569c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def hypergraph_random_walk_matrix(H, start_nodes, walk_length):\n",
    "    \"\"\"\n",
    "    Perform random walks on a hypergraph represented by an adjacency matrix.\n",
    "\n",
    "    Parameters:\n",
    "    H (np.array): Hypergraph adjacency matrix of shape (num_nodes, num_nodes).\n",
    "    start_nodes (list of int): List of starting nodes for the random walks.\n",
    "    walk_length (int): The length of each walk.\n",
    "\n",
    "    Returns:\n",
    "    walk_list (list of lists): List of walks, where each walk is a list of nodes visited.\n",
    "    \"\"\"\n",
    "    num_nodes = H.shape[0]\n",
    "    walk_list = []\n",
    "\n",
    "    for start_node in start_nodes:\n",
    "        walk = [start_node]\n",
    "        current_node = start_node\n",
    "\n",
    "        for _ in range(walk_length):\n",
    "            # Find neighbors of the current node (nodes connected by a hyperedge)\n",
    "            neighbors = np.nonzero(H[current_node])[0]\n",
    "            \n",
    "            if len(neighbors) == 0:\n",
    "                break  # If no neighbors, terminate the walk\n",
    "            \n",
    "            # Choose a random neighbor as the next node\n",
    "            next_node = random.choice(neighbors)\n",
    "            \n",
    "            walk.append(next_node)\n",
    "            current_node = next_node\n",
    "\n",
    "        walk_list.append(torch.Tensor(walk).long().to(DEVICE))\n",
    "    \n",
    "    return walk_list\n",
    "\n",
    "# Example Usage:\n",
    "# Define a hypergraph as a list of hyperedges\n",
    "def edge_index_to_hypergraph_adjacency(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Convert edge index to hypergraph adjacency matrix.\n",
    "\n",
    "    Parameters:\n",
    "    edge_index (np.array): Array of shape (2, num_edges) representing the edges in the graph.\n",
    "    num_nodes (int): Number of nodes in the hypergraph.\n",
    "\n",
    "    Returns:\n",
    "    H (np.array): Hypergraph adjacency matrix of shape (num_nodes, num_nodes).\n",
    "    \"\"\"\n",
    "    # Initialize the adjacency matrix with zeros\n",
    "    H = np.zeros((num_nodes, num_nodes), dtype=int)\n",
    "\n",
    "    # Iterate over each edge and update the adjacency matrix\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1, node2 = edge_index[:, i]\n",
    "        H[node1, node2] = 1\n",
    "        H[node2, node1] = 1  # Since the graph is undirected\n",
    "\n",
    "    return H\n",
    "\n",
    "# Perform random walks for a list of starting nodes [0, 2, 4] with a walk length of 5\n",
    "split_ratio = 0.1\n",
    "walk_length = 30\n",
    "all_random_node_list = torch.randperm(data.num_nodes)\n",
    "selected_node_num_for_random_walk = int(split_ratio * data.num_nodes)\n",
    "# 这一行\n",
    "random_node_list = all_random_node_list[:selected_node_num_for_random_walk].to(DEVICE)\n",
    "walk_list = hypergraph_random_walk_matrix(H, start_nodes=random_node_list, walk_length=walk_length)\n",
    "graph_list = [] \n",
    "skip_num = 0        \n",
    "for walk in walk_list:   \n",
    "    subgraph_nodes = torch.unique(walk).to(DEVICE)\n",
    "    if(len(subgraph_nodes)<5):\n",
    "        skip_num+=1\n",
    "        continue\n",
    "    data = data.to(DEVICE)\n",
    "    #print(subgraph_nodes)\n",
    "    subgraph_data = data.subgraph(subgraph_nodes)\n",
    "    subgraph_data.edge_index = torch.Tensor(edge_index_to_hypergraph_adjacency(subgraph_data.edge_index, len(subgraph_data.x))).to(DEVICE)\n",
    "    graph_list.append(subgraph_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaff000-b4db-4bee-b4a2-a7b58978c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafc00f-441f-4f71-b7f1-6cceb7b4a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from prompt_graph.model import GAT, GCN, GCov, GIN, GraphSAGE, GraphTransformer, HGNN\n",
    "#graph_dataloader = DataLoader([data], batch_size=1, shuffle=True)\n",
    "graph_dataloader = DataLoader(graph_list, batch_size=1, shuffle=True)\n",
    "graph_n_feat_dim = input_dim\n",
    "hid_dim = 128\n",
    "gnn = GCN(input_dim = input_dim, hid_dim = hid_dim, num_layer = 2)\n",
    "\n",
    "decoder = initialize_gnn_decoder(\"HGNN\",hid_dim,input_dim,2,DEVICE)\n",
    "mask_rate = 0.75\n",
    "drop_edge_rate=0.0\n",
    "replace_rate=0.\n",
    "loss_fn='sce'\n",
    "alpha_l=2\n",
    "hid_dim = 128\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00005\n",
    "epochs = 1000\n",
    "\n",
    "mae_loss = GraphMAELoss(gnn, decoder, hid_dim, graph_n_feat_dim, hid_dim, mask_rate, drop_edge_rate, replace_rate, loss_fn, alpha_l).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, list(gnn.parameters()) + list(decoder.parameters())),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7076886-4cbd-43be-bf8c-3d24a76c2cb1",
   "metadata": {},
   "source": [
    "# Pretrain AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db74138-8bb7-49b4-b17e-833fde7c7ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.inits import reset, uniform\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from prompt_graph.utils import generate_corrupted_graph\n",
    "from prompt_graph.data import load4node, load4graph, NodePretrain\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from prompt_graph.model import GAT, GCN, GCov, GIN, GraphSAGE, GraphTransformer\n",
    "from torchmetrics import MeanMetric\n",
    "import numpy as np\n",
    "\n",
    "loss_metric = MeanMetric()\n",
    "train_loss_min = np.inf\n",
    "patience = 50\n",
    "cnt_wait = 0\n",
    "for epoch in range(epochs):\n",
    "    st_time = time.time()\n",
    "    \n",
    "    loss_metric.reset()\n",
    "    \n",
    "    for step, batch in enumerate(graph_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(DEVICE)\n",
    "        loss, loss_item, x_hidden = mae_loss.forward(batch)              \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        loss_metric.update(loss.item(), batch.size(0))\n",
    "    print(f\"GraphMAE [Pretrain] Epoch {epoch}/{epochs} | Train Loss {loss_metric.compute():.5f} | \"\n",
    "          f\"Cost Time {time.time() - st_time:.3}s\")\n",
    "    \n",
    "    if train_loss_min > loss_metric.compute():\n",
    "        train_loss_min = loss_metric.compute()\n",
    "        cnt_wait = 0\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "        if cnt_wait == patience:\n",
    "            print('-' * 100)\n",
    "            print('Early stopping at '+str(epoch) +' eopch!')\n",
    "            break\n",
    "    print(cnt_wait)\n",
    "folder_path = f\"./Experiment/pre_trained_model/ADNI\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "torch.save(gnn.state_dict(),\n",
    "            \"./Experiment/pre_trained_model/{}/{}.{}.{}.pth\".format(\"ADNI\", 'GraphMAE', \"HGCN1\", str(hid_dim) + 'hidden_dim'))\n",
    "\n",
    "print(\"+++model saved ! {}/{}.{}.{}.pth\".format(\"ADNI\", 'GraphMAE', \"HGCN\", str(hid_dim) + 'hidden_dim'))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a47fa8-b652-4fd1-ae06-61f72cf57a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from prompt_graph.model import GAT, GCN, GCov, GIN, GraphSAGE, GraphTransformer, HGNN\n",
    "#graph_dataloader = DataLoader([data], batch_size=1, shuffle=True)\n",
    "graph_dataloader = DataLoader(graph_list, batch_size=1, shuffle=True)\n",
    "graph_n_feat_dim = input_dim\n",
    "hid_dim = 128\n",
    "# gnn = HGNN(input_dim = input_dim, hid_dim = hid_dim, out_dim=2, num_layer = 2)\n",
    "\n",
    "# decoder = HGNN(input_dim = 2, hid_dim = hid_dim, out_dim=input_dim, num_layer = 2)\n",
    "gnn = HGNN(input_dim = input_dim, hid_dim = hid_dim, out_dim=hid_dim, num_layer = 2)\n",
    "decoder = HGNN(input_dim = hid_dim, hid_dim = hid_dim, out_dim=input_dim, num_layer = 2)\n",
    "#initialize_gnn_decoder(\"HGNN\",2,input_dim,input_dim,DEVICE)\n",
    "mask_rate = 0.75\n",
    "drop_edge_rate=0.0\n",
    "replace_rate=0.1\n",
    "loss_fn='sce'\n",
    "alpha_l=2\n",
    "hid_dim = 128\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00005\n",
    "epochs = 1000\n",
    "mae_loss = GraphMAELoss(gnn, decoder, hid_dim, graph_n_feat_dim, 128, mask_rate, drop_edge_rate, replace_rate, loss_fn, alpha_l).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, list(gnn.parameters()) + list(decoder.parameters())),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2081bbc-b890-431c-bcc2-a9c4c97db2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.inits import reset, uniform\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from prompt_graph.utils import generate_corrupted_graph\n",
    "from prompt_graph.data import load4node, load4graph, NodePretrain\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from prompt_graph.model import GAT, GCN, GCov, GIN, GraphSAGE, GraphTransformer\n",
    "from torchmetrics import MeanMetric\n",
    "import numpy as np\n",
    "\n",
    "loss_metric = MeanMetric()\n",
    "train_loss_min = np.inf\n",
    "patience = 50\n",
    "cnt_wait = 0\n",
    "for epoch in range(epochs):\n",
    "    st_time = time.time()\n",
    "    \n",
    "    loss_metric.reset()\n",
    "    \n",
    "    for step, batch in enumerate(graph_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(DEVICE)\n",
    "        loss, loss_item, x_hidden = mae_loss.forward(batch)              \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        loss_metric.update(loss.item(), batch.size(0))\n",
    "    print(f\"GraphMAE [Pretrain] Epoch {epoch}/{epochs} | Train Loss {loss_metric.compute():.5f} | \"\n",
    "          f\"Cost Time {time.time() - st_time:.3}s\")\n",
    "    \n",
    "    if train_loss_min > loss_metric.compute():\n",
    "        train_loss_min = loss_metric.compute()\n",
    "        cnt_wait = 0\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "        if cnt_wait == patience:\n",
    "            print('-' * 100)\n",
    "            print('Early stopping at '+str(epoch) +' eopch!')\n",
    "            break\n",
    "    print(cnt_wait)\n",
    "folder_path = f\"./Experiment/pre_trained_model/ADNI_MCI\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "torch.save(gnn.state_dict(),\n",
    "            \"./Experiment/pre_trained_model/{}/{}.{}.{}.pth\".format(\"ADNI_MCI\", 'GraphMAE', \"128HGNN_mri\", str(hid_dim) + 'hidden_dim'))\n",
    "\n",
    "print(\"+++model saved ! {}/{}.{}.{}.pth\".format(\"ADNI_MCI\", 'GraphMAE', \"HGNN\", str(hid_dim) + 'hidden_dim'))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c14743-dfa6-49da-98b6-47a930242ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361d296-2152-4e05-aaf7-ba8613e7da73",
   "metadata": {},
   "source": [
    "# Gprompt Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578794c-604d-4024-9c62-cd2860daa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_graph.tasker import NodeTask, GraphTask\n",
    "from prompt_graph.utils import seed_everything\n",
    "from torchsummary import summary\n",
    "from prompt_graph.utils import print_model_parameters\n",
    "from prompt_graph.utils import  get_args\n",
    "from prompt_graph.data import load4node,load4graph, split_induced_graphs\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_induced_graph(dataset_name, data, device):\n",
    "\n",
    "    folder_path = './Experiment/induced_graph/' + dataset_name\n",
    "    if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    file_path = folder_path + '/induced_graph_min100_max300.pkl'\n",
    "    if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                print('loading induced graph...')\n",
    "                graphs_list = pickle.load(f)\n",
    "                print('Done!!!')\n",
    "    else:\n",
    "        print('Begin split_induced_graphs.')\n",
    "        split_induced_graphs(data, folder_path, device, smallest_size=100, largest_size=300)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            graphs_list = pickle.load(f)\n",
    "    graphs_list = [graph.to(device) for graph in graphs_list]\n",
    "    return graphs_list\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83467054-e56b-4859-8925-d9d30e80459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a04f4-51aa-4cee-9383-a677c0042253",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = np.transpose(np.transpose(np.nonzero(H)))\n",
    "data = Data(x=stacked_outputs.to(DEVICE), edge_index=torch.Tensor(edge_indices).long().to(DEVICE), y=labels.squeeze().long(), train_mask=train_mask, val_mask = ~train_mask, test_mask = ~train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de1c8f-2769-4b86-ab9f-bf63c6108086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index_to_hypergraph_adjacency(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Convert edge index to hypergraph adjacency matrix.\n",
    "\n",
    "    Parameters:\n",
    "    edge_index (np.array): Array of shape (2, num_edges) representing the edges in the graph.\n",
    "    num_nodes (int): Number of nodes in the hypergraph.\n",
    "\n",
    "    Returns:\n",
    "    H (np.array): Hypergraph adjacency matrix of shape (num_nodes, num_nodes).\n",
    "    \"\"\"\n",
    "    # Initialize the adjacency matrix with zeros\n",
    "    H = np.zeros((num_nodes, num_nodes), dtype=int)\n",
    "\n",
    "    # Iterate over each edge and update the adjacency matrix\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1, node2 = edge_index[:, i]\n",
    "        H[node1, node2] = 1\n",
    "        H[node2, node1] = 1  # Since the graph is undirected\n",
    "\n",
    "    return H\n",
    "dataset_name = \"ADNI_MCI\"\n",
    "prompt_type = 'GPF-plus'\n",
    "if prompt_type in ['Gprompt', 'All-in-one', 'GPF', 'GPF-plus']:\n",
    "    graphs_list = load_induced_graph(dataset_name, data, DEVICE) \n",
    "else:\n",
    "    graphs_list = None \n",
    "for graph in graphs_list:   \n",
    "   \n",
    "    graph.edge_index = torch.Tensor(edge_index_to_hypergraph_adjacency(graph.edge_index, len(graph.x))).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe1797-fda6-433f-8e52-04ed40ac5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获取标签\n",
    "labels = data.y\n",
    "\n",
    "# 使用 train_mask 生成训练集索引\n",
    "train_mask = data.train_mask\n",
    "train_indices = torch.nonzero(train_mask).view(-1)\n",
    "\n",
    "# 使用 ~train_mask 生成测试集索引\n",
    "test_mask = ~train_mask\n",
    "test_indices = torch.nonzero(test_mask).view(-1)\n",
    "\n",
    "\n",
    "folder = \"./Experiment/sample_data/Node/ADNI_MCI/11_shot/1\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "# 保存训练集索引和标签\n",
    "torch.save(train_indices, os.path.join(folder, 'train_idx.pt'))\n",
    "train_labels = labels[train_indices]\n",
    "torch.save(train_labels, os.path.join(folder, 'train_labels.pt'))\n",
    "\n",
    "# 保存测试集索引和标签\n",
    "torch.save(test_indices, os.path.join(folder, 'test_idx.pt'))\n",
    "test_labels = labels[test_indices]\n",
    "torch.save(test_labels, os.path.join(folder, 'test_labels.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb7925-540a-43e4-809b-374d20f1f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_model_path = \"./Experiment/pre_trained_model/ADNI/GraphMAE.HGCN.128hidden_dim.pth\"\n",
    "num_layer = 2\n",
    "gnn_type = \"GCN\"\n",
    "hid_dim = 128\n",
    "prompt_type = \"GPF-plus\"\n",
    "epochs = 1000\n",
    "shot_num = 10\n",
    "lr = 0.00001\n",
    "decay = 2e-6\n",
    "batch_size = 128\n",
    "tasker = NodeTask(pre_train_model_path = pre_train_model_path, \n",
    "                    dataset_name = dataset_name, num_layer = num_layer,\n",
    "                    gnn_type = gnn_type, hid_dim = hid_dim, prompt_type = prompt_type,\n",
    "                    epochs = epochs, shot_num = 11, device='2', lr = lr, wd = decay,\n",
    "                    batch_size = batch_size, data = data, input_dim = input_dim, output_dim = output_dim, graphs_list = graphs_list)\n",
    "\n",
    "pre_train_type = tasker.pre_train_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021f58b-be66-4356-9683-04c3712a8fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, test_acc, std_test_acc, f1, std_f1, roc, std_roc, _, _= tasker.run()\n",
    "  \n",
    "print(\"Final Accuracy {:.4f}±{:.4f}(std)\".format(test_acc, std_test_acc)) \n",
    "print(\"Final F1 {:.4f}±{:.4f}(std)\".format(f1,std_f1)) \n",
    "print(\"Final AUROC {:.4f}±{:.4f}(std)\".format(roc, std_roc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d83f54-99b8-48dc-bb71-fc3d4748f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_model_path = \"./Experiment/pre_trained_model/ADNI_MCI/GraphMAE.32HGNN.128hidden_dim.pth\"\n",
    "num_layer = 2\n",
    "gnn_type = \"HGNN\"\n",
    "hid_dim = 32\n",
    "\n",
    "prompt_type = \"GPF-plus\"\n",
    "epochs = 1000\n",
    "shot_num = 10\n",
    "lr = 0.00001\n",
    "decay = 2e-6\n",
    "batch_size = 1\n",
    "tasker = NodeTask(pre_train_model_path = pre_train_model_path, \n",
    "                    dataset_name = dataset_name, num_layer = num_layer,\n",
    "                    gnn_type = gnn_type, hid_dim = hid_dim, prompt_type = prompt_type,\n",
    "                    epochs = epochs, shot_num = 11, device='3', lr = lr, wd = decay,\n",
    "                    batch_size = batch_size, data = data, input_dim = input_dim, output_dim = 2, graphs_list = graphs_list)\n",
    "\n",
    "pre_train_type = tasker.pre_train_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe49c0-c6c3-46be-8e31-9caeb279d3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "_, test_acc, std_test_acc, f1, std_f1, roc, std_roc, _, _= tasker.run()\n",
    "print(\"Final Accuracy {:.4f}±{:.4f}(std)\".format(test_acc, std_test_acc)) \n",
    "print(\"Final F1 {:.4f}±{:.4f}(std)\".format(f1,std_f1)) \n",
    "print(\"Final AUROC {:.4f}±{:.4f}(std)\".format(roc, std_roc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93519cc4-7e34-4bfc-956c-ee652ddfa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasker.gnn = HGNN(input_dim = input_dim, hid_dim = hid_dim, out_dim=2, num_layer = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b7067-ab8b-40d8-a431-fe6518379027",
   "metadata": {},
   "source": [
    "# HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3b3e0-f4ab-4e44-89e9-0e8583d612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_feature = stacked_outputs[:,:512]\n",
    "pet_feature = stacked_outputs[:,512:1024]\n",
    "clinical_feature = stacked_outputs[:,1024:]\n",
    "\n",
    "from hypergraph_utils import *\n",
    "mri_dis_mat = Eu_dis(mri_feature.cpu())\n",
    "pet_dis_mat = Eu_dis(pet_feature.cpu())\n",
    "clinical_dis_mat = Eu_dis(clinical_feature.cpu())\n",
    "\n",
    "k_distance = 30\n",
    "\n",
    "mri_H = construct_H_with_KNN_from_distance(mri_dis_mat, k_distance, is_probH=False, m_prob=1)\n",
    "pet_H = construct_H_with_KNN_from_distance(pet_dis_mat, k_distance, is_probH=False, m_prob=1)\n",
    "clinical_H = construct_H_with_KNN_from_distance(clinical_dis_mat, k_distance, is_probH=False, m_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cbe8c5-5cda-49ca-af8f-efbc932cb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#H_list = [mri_H, pet_H, clinical_H]\n",
    "H_list = [mri_H]\n",
    "H = None\n",
    "for h in H_list:\n",
    "    if h is not None:\n",
    "        # for the first H appended to fused hypergraph incidence matrix\n",
    "        if H is None:\n",
    "            H = h\n",
    "        else:\n",
    "            if type(h) != list:\n",
    "                H = np.hstack((H, h))\n",
    "            else:\n",
    "                tmp = []\n",
    "                for a, b in zip(H, h):\n",
    "                    tmp.append(np.hstack((a, b)))\n",
    "                H = tmp\n",
    "\n",
    "G = generate_G_from_H(H)\n",
    "G = torch.Tensor(G).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8120a-cb86-47af-a640-6886f9724dbd",
   "metadata": {},
   "source": [
    "# HPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d51f8-505c-4d39-a4df-f52faa7f2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGPrompt(torch.nn.Module):\n",
    "    def __init__(self, token_num: int, token_dim: int):\n",
    "        super(HGPrompt, self).__init__()\n",
    "        self.tokens = torch.nn.Parameter(torch.Tensor(token_num, token_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.tokens, nonlinearity='leaky_relu', mode='fan_in', a=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c14d9-c756-4a13-9f3f-eb256a2baf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num = 2\n",
    "token_dim = 1024\n",
    "prompt = HGPrompt(token_num, token_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc1510-3d50-4b9d-81bb-89d6682d1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的维度\n",
    "new_rows = H.shape[0] + token_num\n",
    "new_cols = H.shape[1] + token_num * 3\n",
    "\n",
    "# 创建一个形状为 (new_rows, new_cols) 的全1张量\n",
    "H_expanded = np.ones((new_rows, new_cols))\n",
    "\n",
    "# 将原始的H张量复制到扩充后的张量中\n",
    "H_expanded[:H.shape[0], :H.shape[1]] = H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7976a-128b-4d2d-b0bb-480a57c447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dis_mat = Eu_dis(prompt.tokens.detach().cpu())\n",
    "\n",
    "k_distance = token_num // 2\n",
    "\n",
    "prompt_H = construct_H_with_KNN_from_distance(prompt_dis_mat, k_distance, is_probH=False, m_prob=1)\n",
    "H_expanded[H.shape[0]:, H.shape[1]:H.shape[1]+token_num] = prompt_H\n",
    "H_expanded[H.shape[0]:, H.shape[1]+token_num:H.shape[1]+token_num*2] = prompt_H\n",
    "H_expanded[H.shape[0]:, H.shape[1]+token_num*2:H.shape[1]+token_num*3] = prompt_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d201d7b-d9c1-4620-987e-187151df1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generate_G_from_H(H_expanded)\n",
    "G = torch.Tensor(G).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c5414-655d-4156-a40a-8dad62e6c0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import OrderedDict\n",
    "from models import HGNN\n",
    "input = clinical_feature\n",
    "input = torch.cat([mri_feature, pet_feature], dim=1)\n",
    "input = torch.cat([mri_feature, clinical_feature], dim=1)\n",
    "input = torch.cat([pet_feature, clinical_feature], dim=1)\n",
    "prompt = prompt.to(DEVICE)\n",
    "prompted_input = torch.cat([input, prompt.tokens])\n",
    "hgnn = HGNN(in_ch=prompted_input.shape[1],\n",
    "                    n_class=2,\n",
    "                    n_hid=256,\n",
    "                    dropout=0.0)\n",
    "# state_dict = torch.load(\"./Experiment/pre_trained_model/ADNI_MCI/GraphMAE.HGNN.128hidden_dim.pth\")\n",
    "# state_dict = OrderedDict({\n",
    "#     'hgc1.weight': state_dict['conv_layers.0.weight'],\n",
    "#     'hgc1.bias': state_dict['conv_layers.0.bias'],\n",
    "#     'hgc2.weight': state_dict['conv_layers.1.weight'],\n",
    "#     'hgc2.bias': state_dict['conv_layers.1.bias'],\n",
    "#     # Add more mappings as needed\n",
    "# })\n",
    "\n",
    "# hgnn.load_state_dict(state_dict)\n",
    "hgnn = torch.load(\"./weights/concat_pet_clinical_resnet50_hgnn_mci.pth\")\n",
    "hgnn = hgnn.to(DEVICE)\n",
    "prompt = HGPrompt(token_num, token_dim).to(DEVICE)\n",
    "G = torch.Tensor(G).to(DEVICE)\n",
    "prompted_input = prompted_input.to(DEVICE)\n",
    "# here\n",
    "optimizer = optim.AdamW(prompt.parameters(), lr=3e-3, weight_decay=0)\n",
    "\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_fig = []\n",
    "eva_fig = []\n",
    "\n",
    "epochs = 1000\n",
    "best_auc = 0\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # here\n",
    "    hgnn.eval()\n",
    "    prompt.train()\n",
    "\n",
    "    prompt_dis_mat = Eu_dis(prompt.tokens.detach().cpu())\n",
    "\n",
    "\n",
    "    \n",
    "    prompt_H = construct_H_with_KNN_from_distance(prompt_dis_mat, k_distance, is_probH=False, m_prob=1)\n",
    "    H_expanded[H.shape[0]:, H.shape[1]:H.shape[1]+token_num] = prompt_H\n",
    "    H_expanded[H.shape[0]:, H.shape[1]+token_num:H.shape[1]+token_num*2] = prompt_H\n",
    "    H_expanded[H.shape[0]:, H.shape[1]+token_num*2:H.shape[1]+token_num*3] = prompt_H\n",
    "    G = generate_G_from_H(H_expanded)\n",
    "    G = torch.Tensor(G).to(DEVICE)\n",
    "    #prompt.tokens = prompt.tokens.to(DEVICE)\n",
    "    prompted_input = torch.cat([input, prompt.tokens])\n",
    "\n",
    "    \n",
    "    pred = hgnn(prompted_input, G)[:-token_num]\n",
    "    loss = loss_function(pred[train_mask].to(DEVICE), labels[train_mask].squeeze(1).to(DEVICE).long())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    tqdm.write(\"Epoch: {}/{}, train loss: {}\".format(epoch, epochs, round(loss.item(), 5)))\n",
    "    # filein.write(\"Epoch: {}/{}, train loss: {}\\n\".format(i, epochs, round(loss, 5)))\n",
    "    loss_fig.append(round(loss.item(), 5))\n",
    "    # here\n",
    "    hgnn.eval()\n",
    "    prompt.eval()\n",
    "    # here\n",
    "    logits = hgnn(prompted_input, G)[:-token_num]\n",
    "    mask_logits = logits[~train_mask]\n",
    "        \n",
    "    predicted = F.softmax(mask_logits, dim=1)\n",
    "    label = labels[~train_mask]\n",
    "    predicted = predicted.cpu().detach().numpy()\n",
    "    label = label.cpu().detach().numpy()\n",
    "    \n",
    "    accuracy, sensitivity, specificity, roc_auc = evaluate_gnn(label, predicted)\n",
    "    eva_fig.append(accuracy)\n",
    "    tqdm.write(\"Epoch: {}/{}, evaluation loss: {}\".format(epoch, epochs,(accuracy, sensitivity, specificity, roc_auc)))\n",
    "    if roc_auc > best_auc:\n",
    "        print(f\"save pth in epoch {epoch}\")\n",
    "        best_auc = roc_auc\n",
    "        best_epoch = epoch\n",
    "        # here\n",
    "        torch.save(hgnn, \"./weights/prompt_concat_pet_clinical_resnet50_hgnn_mci.pth\")\n",
    "\n",
    "    print(best_epoch)\n",
    "    print(best_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98919216-c6ff-472b-85f7-c60dcf4f8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([mri_feature, pet_feature], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f16e4e-5891-4a0d-8395-60a1e89d62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a6c0c-4f41-4eac-9228-ee10e32820f3",
   "metadata": {},
   "source": [
    "## HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc505c8-e70a-417a-af0b-0526ddd58418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import OrderedDict\n",
    "from models import HGNN\n",
    "input = clinical_feature\n",
    "input = torch.cat([mri_feature, pet_feature], dim=1)\n",
    "input = torch.cat([mri_feature, clinical_feature], dim=1)\n",
    "input = torch.cat([pet_feature, clinical_feature], dim=1)\n",
    "input = mri_feature\n",
    "hgnn = HGNN(in_ch=input.shape[1],\n",
    "                    n_class=2,\n",
    "                    n_hid=256,\n",
    "                    dropout=0.0)\n",
    "# state_dict = torch.load(\"./Experiment/pre_trained_model/ADNI_MCI/GraphMAE.HGNN.128hidden_dim.pth\")\n",
    "# state_dict = OrderedDict({\n",
    "#     'hgc1.weight': state_dict['conv_layers.0.weight'],\n",
    "#     'hgc1.bias': state_dict['conv_layers.0.bias'],\n",
    "#     'hgc2.weight': state_dict['conv_layers.1.weight'],\n",
    "#     'hgc2.bias': state_dict['conv_layers.1.bias'],\n",
    "#     # Add more mappings as needed\n",
    "# })\n",
    "\n",
    "# hgnn.load_state_dict(state_dict)\n",
    "hgnn = hgnn.to(DEVICE)\n",
    "input = input.to(DEVICE)\n",
    "G = torch.Tensor(G).to(DEVICE)\n",
    "\n",
    "# here\n",
    "optimizer = optim.AdamW(hgnn.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_fig = []\n",
    "eva_fig = []\n",
    "\n",
    "epochs = 8000\n",
    "best_auc = 0\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # here\n",
    "    hgnn.train()\n",
    "\n",
    "    pred = hgnn(input, G)\n",
    "    loss = loss_function(pred[train_mask].to(DEVICE), labels[train_mask].to(DEVICE).squeeze(1).long())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    tqdm.write(\"Epoch: {}/{}, train loss: {}\".format(epoch, epochs, round(loss.item(), 5)))\n",
    "    # filein.write(\"Epoch: {}/{}, train loss: {}\\n\".format(i, epochs, round(loss, 5)))\n",
    "    loss_fig.append(round(loss.item(), 5))\n",
    "    # here\n",
    "    hgnn.eval()\n",
    "\n",
    "    # here\n",
    "    logits = hgnn(input, G)\n",
    "    mask_logits = logits[~train_mask]\n",
    "        \n",
    "    predicted = F.softmax(mask_logits, dim=1)\n",
    "    label = labels[~train_mask]\n",
    "    predicted = predicted.cpu().detach().numpy()\n",
    "    label = label.cpu().detach().numpy()\n",
    "    \n",
    "    accuracy, sensitivity, specificity, roc_auc = evaluate_gnn(label, predicted)\n",
    "    eva_fig.append(accuracy)\n",
    "    tqdm.write(\"Epoch: {}/{}, evaluation loss: {}\".format(epoch, epochs,(accuracy, sensitivity, specificity, roc_auc)))\n",
    "    if roc_auc > best_auc:\n",
    "        print(f\"save pth in epoch {epoch}\")\n",
    "        best_auc = roc_auc\n",
    "        best_epoch = epoch\n",
    "        # here\n",
    "        torch.save(hgnn, \"./weights/concat_mri_resnet50_hgnn_mci.pth\")\n",
    "\n",
    "    print(best_epoch)\n",
    "    print(best_auc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibm",
   "language": "python",
   "name": "bibm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
